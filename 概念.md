[![千分尺](http://micrometer.io/static/media/logo.65805db1.svg)](http://micrometer.io/)

- [ 文献资料](http://micrometer.io/docs)
- [ 的GitHub](https://github.com/micrometer-metrics/micrometer)
- [ 推特](https://twitter.com/micrometerio)
- [ 松弛](https://join.slack.com/t/micrometer-metrics/shared_invite/zt-ewo3kcs0-Ji3aOAqTxnjYPEFBBI5HqQ)

# 概念

------

目录

- [1。目的](http://micrometer.io/docs/concepts#_purpose)
- [2.支持的监控系统](http://micrometer.io/docs/concepts#_supported_monitoring_systems)
- 3.注册表
  - [3.1。复合注册表](http://micrometer.io/docs/concepts#_composite_registries)
  - [3.2。全球注册](http://micrometer.io/docs/concepts#_global_registry)
- [4.米](http://micrometer.io/docs/concepts#_meters)
- 5.命名仪表
  - [5.1。标签命名](http://micrometer.io/docs/concepts#_tag_naming)
  - [5.2。常用标签](http://micrometer.io/docs/concepts#_common_tags)
  - [5.3。标签值](http://micrometer.io/docs/concepts#_tag_values)
- 6.仪表过滤器
  - [6.1。拒绝/接受米](http://micrometer.io/docs/concepts#_denyaccept_meters)
  - [6.2。转换指标](http://micrometer.io/docs/concepts#_transforming_metrics)
  - [6.3。配置分布统计](http://micrometer.io/docs/concepts#_configuring_distribution_statistics)
- 7.费率汇总
  - [7.1。服务器端](http://micrometer.io/docs/concepts#_server_side)
  - [7.2。客户端](http://micrometer.io/docs/concepts#_client_side)
- 8.柜台
  - [8.1。功能跟踪计数器](http://micrometer.io/docs/concepts#_function_tracking_counters)
- 9.量规
  - [9.1。手动增减仪表](http://micrometer.io/docs/concepts#_manually_incrementingdecrementing_a_gauge)
  - [9.2。量规流利的建设者](http://micrometer.io/docs/concepts#_gauge_fluent_builder)
  - [9.3。我的仪表为什么报告NaN或消失？](http://micrometer.io/docs/concepts#_why_is_my_gauge_reporting_nan_or_disappearing)
- 10.计时器
  - [10.1。记录代码块](http://micrometer.io/docs/concepts#_recording_blocks_of_code)
  - [10.2。将开始状态存储在`Timer.Sample`](http://micrometer.io/docs/concepts#_storing_start_state_in_timer_sample)
  - [10.3。该`@Timed`注解](http://micrometer.io/docs/concepts#_the_timed_annotation)
  - [10.4。功能跟踪计时器](http://micrometer.io/docs/concepts#_function_tracking_timers)
  - [10.5。暂停检测](http://micrometer.io/docs/concepts#_pause_detection)
  - [10.6。内存占用估算](http://micrometer.io/docs/concepts#_memory_footprint_estimation)
- 11.发行摘要
  - [11.1。标度和直方图](http://micrometer.io/docs/concepts#_scaling_and_histograms)
  - [11.2。内存占用估算](http://micrometer.io/docs/concepts#_memory_footprint_estimation_2)
- [12.长任务计时器](http://micrometer.io/docs/concepts#_long_task_timers)
- [13. Histograms and percentiles](http://micrometer.io/docs/concepts#_histograms_and_percentiles)

## 1. Purpose

Micrometer is a metrics instrumentation library for JVM-based applications. It provides a simple facade over the instrumentation clients for the most popular monitoring systems, allowing you to instrument your JVM-based application code without vendor lock-in. It is designed to add little to no overhead to your metrics collection activity while maximizing the portability of your metrics effort.

Micrometer is *not* a distributed tracing system or an event logger. Adrian Cole’s talk on [Observability 3 Ways](https://www.dotconferences.com/2017/04/adrian-cole-observability-3-ways-logging-metrics-tracing) does a great job of highlighting the differences between these different types of systems.

## 2. Supported monitoring systems

Micrometer contains a core module with an instrumentation [SPI](https://en.wikipedia.org/wiki/Service_provider_interface), a set of modules containing implementations for various monitoring systems (each is called a registry), and a test kit. There are three important characteristics of monitoring systems that it is important to understand:

- **Dimensionality**. Whether the system supports metric names to be enriched with tag key/value pairs. If a system is not dimensional, it is *hierarchical*, which means it only supports a flat metric name. When publishing metrics to hierarchical systems, Micrometer flattens the set of tag key/value pairs and adds them to the name.

| Dimensional                                                  | Hierarchical                        |
| :----------------------------------------------------------- | :---------------------------------- |
| AppOptics, Atlas, Azure Monitor, Cloudwatch, Datadog, Datadog StatsD, Dynatrace, Elastic, Humio, Influx, KairosDB, New Relic, Prometheus, SignalFx, Sysdig StatsD, Telegraf StatsD, Wavefront | Graphite, Ganglia, JMX, Etsy StatsD |

- **[Rate aggregation](http://micrometer.io/docs/concepts#rate-aggregation)**. In this context, we mean aggregation of a set of samples over a prescribed time interval. Some monitoring systems expect for some types of discrete samples (such as counts) to be converted to a rate by the application prior to being published. Some expect cumulative values to always be sent. And still others have no opinion on it either way.

| Client-side                                                  | Server-side                                                  |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| AppOptics, Atlas, Azure Monitor, Datadog, Elastic, Graphite, Ganglia, Humio, Influx, JMX, Kairos, New Relic, all StatsD flavors, SignalFx | Prometheus, Wavefront [[1](http://micrometer.io/docs/concepts#_footnotedef_1)] |

- **Publishing**. Some systems expect to poll applications for metrics at their leisure, and others expect metrics to be pushed to them on a regular interval.

| Client pushes                                                | Server polls                   |
| :----------------------------------------------------------- | :----------------------------- |
| AppOptics, Atlas, Azure Monitor, Datadog, Elastic, Graphite, Ganglia, Humio, Influx, JMX, Kairos, New Relic, SignalFx, Wavefront | Prometheus, all StatsD flavors |

There are other, more minor, variations in expectations from one monitoring system to another, such as their conception of base units of measurement (particularly time) and the canonical naming convention for metrics. Micrometer customizes your metrics to meet these demands as well on a per-registry basis.

## 3. Registry

A `Meter` is the interface for collecting a set of measurements (which we individually call metrics) about your application. Meters in Micrometer are created from and held in a `MeterRegistry`. Each supported monitoring system has an implementation of `MeterRegistry`. How a registry is created varies for each implementation.

Micrometer packs with a `SimpleMeterRegistry` that holds the latest value of each meter in memory and doesn’t export the data anywhere. If you don’t yet have a preferred monitoring system, you can get started playing with metrics by using the simple registry:

```java
MeterRegistry registry = new SimpleMeterRegistry();
```

| NOTE | A `SimpleMeterRegistry` is autowired for you in Spring-based apps. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 3.1. Composite registries

Micrometer provides a `CompositeMeterRegistry` to which multiple registries can be added, allowing you to publish metrics to more than one monitoring system simultaneously.

```java
CompositeMeterRegistry composite = new CompositeMeterRegistry();

Counter compositeCounter = composite.counter("counter");
compositeCounter.increment(); (1)

SimpleMeterRegistry simple = new SimpleMeterRegistry();
composite.add(simple); (2)

compositeCounter.increment(); (3)
```

1. Increments are NOOPd until there is a registry in the composite. The counter’s count will still yield 0 at this point.
2. A counter named "counter" is registered to the simple registry.
3. The simple registry counter is incremented, along with counters for any other registries in the composite.

### 3.2. Global registry

Micrometer provides a static global registry `Metrics.globalRegistry` and a set of static builders for generating meters based on this registry. `globalRegistry` is a composite registry.

```java
class MyComponent {
    Counter featureCounter = Metrics.counter("feature", "region", "test"); (1)

    void feature() {
        featureCounter.increment();
    }

    void feature2(String type) {
        Metrics.counter("feature.2", "type", type).increment(); (2)
    }
}

class MyApplication {
    void start() {
        // wire your monitoring system to global static state
        Metrics.addRegistry(new SimpleMeterRegistry()); (3)
    }
}
```

1. Wherever possible (and especially where instrumentation performance is critical), store `Meter` instances in fields to avoid a lookup on their name/tags on each use.
2. When tags need to be determined from local context, you have no choice but to construct/lookup the Meter inside your method body. The lookup cost is just a single hash lookup, so it will be acceptable for most uses.
3. It is OK to add registries *after* meters have been created like `Metrics.counter(…)`. These meters will be added to each registry as it is bound to the global composite.

## 4. Meters

Micrometer packs with a supported set of `Meter` primitives including: `Timer`, `Counter`, `Gauge`, `DistributionSummary`, `LongTaskTimer`, `FunctionCounter`, `FunctionTimer`, and `TimeGauge`. Different meter types result in a different number of time series metrics. For example, while there is a single metric that represents a `Gauge`, a `Timer` measures both the count of timed events and the total time of all events timed.

A meter is uniquely identified by its name and dimensions. We use the term dimensions and tags interchangeably, and the Micrometer interface is `Tag` simply because it is shorter. As a general rule it should be possible to use the name as a pivot. Dimensions allow a particular named metric to be sliced to drill down and reason about the data. This means that if just the name is selected, the user can drill down using other dimensions and reason about the value being shown.

## 5. Naming meters

Micrometer employs a naming convention that separates lowercase words with a '.' (dot) character. Different monitoring systems have different recommendations regarding naming convention, and some naming conventions may be incompatible for one system and not another. Each Micrometer implementation for a monitoring system comes with a naming convention that transforms lowercase dot notation names to the monitoring system’s recommended naming convention. Additionally, this naming convention implementation sanitizes metric names and tags of special characters that are disallowed by the monitoring system. You can override the default naming convention for a registry by implementing `NamingConvention` and setting it on the registry with:

```java
registry.config().namingConvention(myCustomNamingConvention);
```

With naming conventions in place, the following timer registered in Micrometer looks good natively in a wide variety of monitoring systems:

```java
registry.timer("http.server.requests");
```

1. Prometheus - `http_server_requests_duration_seconds`
2. Atlas - `httpServerRequests`
3. Graphite - `http.server.requests`
4. InfluxDB - `http_server_requests`

By adhering to Micrometer’s lowercase dot notation convention, you guarantee the maximum degree of portability for your metric names across monitoring systems.

### 5.1. Tag naming

| TIP  | It is recommended to follow the same lowercase dot notation described for meter names when naming tags. Utilizing this consistent naming convention for tags allows for better translation into the respective monitoring system’s idiomatic naming schemes. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Suppose we are trying to measure the number of http requests and the number of database calls.

**Recommended approach**

```java
registry.counter("database.calls", "db", "users")
registry.counter("http.requests", "uri", "/api/users")
```

This variant provides enough context so that if just the name is selected the value can be reasoned about and is at least potentially meaningful. For example if we select `database.calls` we can see the total number of calls to all databases. Then we can group by or select by `db` to drill down further or perform comparative analysis on the contribution of calls to each database.

**Bad approach**

```java
registry.counter("calls",
    "class", "database",
    "db", "users");

registry.counter("calls",
    "class", "http",
    "uri", "/api/users");
```

In this approach, if we select `calls` we will get a value that is an aggregate of the number of calls to the database and to our API endpoint. This time series is not useful without further dimensional drill-down.

### 5.2. Common tags

Common tags can be defined at the registry level and are added to every metric reported to the monitoring system. This is generally used for dimensional drill-down on the operating environment like host, instance, region, stack, etc.

```java
registry.config().commonTags("stack", "prod", "region", "us-east-1");
registry.config().commonTags(Arrays.asList(Tag.of("stack", "prod"), Tag.of("region", "us-east-1"))); // equivalently
```

Calls to `commonTags` append additional common tags.

| IMPORTANT | If you are in the Spring environment, add common tags by adding a `MeterRegistryCustomizer` bean to be sure that common tags are applied before autoconfigured meter binders. |
| --------- | ------------------------------------------------------------ |
|           |                                                              |

### 5.3. Tag values

Tag values must be non-null.

| WARNING | Beware of the potential for tag values coming from user-supplied sources to blow up the cardinality of a metric. You should always carefully normalize and bound user-supplied input. Sometimes the cause is sneaky. Consider the URI tag for recording HTTP requests on service endpoints. If we don’t constrain 404’s to a value like NOT_FOUND, the dimensionality of the metric would grow with each resource that can’t be found. |
| ------- | ------------------------------------------------------------ |
|         |                                                              |

## 6. Meter filters

Each registry can be configured with meter filters, which allow you to exercise greater control over how and when meters are registered and what kinds of statistics they emit. Meter filters serve three basic functions:

1. **Deny** (or accept) meters from being registered.
2. **Transform** meter IDs (e.g. changing the name, adding or removing tags, changing description or base units).
3. **Configure** distribution statistics for some meter types.

Implementations of `MeterFilter` are added to the registry programmatically:

```java
registry.config()
    .meterFilter(MeterFilter.ignoreTags("too.much.information"))
    .meterFilter(MeterFilter.denyNameStartsWith("jvm"));
```

Meter filters are applied in order and the results of transforming or configuring a meter are chained.

### 6.1. Deny/accept meters

The verbose form of an accept/deny filter is:

```java
new MeterFilter() {
    @Override
    public MeterFilterReply accept(Meter.Id id) {
       if(id.getName().contains("test")) {
          return MeterFilterReply.DENY;
       }
       return MeterFilterReply.NEUTRAL;
    }
}
```

`MeterFilterReply` has three possible states:

- `DENY` - Do not allow this meter to be registered. When you attempt to register a meter against a registry and the filter returns DENY the registry will return a NOOP version of that meter (e.g. `NoopCounter`, `NoopTimer`). Your code can continue to interact with the NOOP meter, but anything recorded to it is discarded immediately with minimal overhead.
- `NEUTRAL` - If no other meter filter has returned `DENY`, then registration of meters proceeds as normal.
- `ACCEPT` - If a filter returns `ACCEPT`, the meter is immediately registered without interrogating any further filters' accept methods.

#### 6.1.1. Convenience methods

`MeterFilter` provides several convenience static builders for deny/accept type filters:

- `accept()` - Accept every meter, overriding the decisions of any filters that follow.
- `accept(Predicate)` - Accept any meter matching the predicate.
- `acceptNameStartsWith(String)` - Accept every meter with a matching prefix.
- `deny()` - Deny every meter, overriding the decisions of any filters that follow.
- `denyNameStartsWith(String)` - Deny every meter with a matching prefix. All out-of-the-box `MeterBinder` implementations provided by Micrometer have names with common prefixes to allow for easy grouping visualization in UIs, but also to make them easy to disable/enable as a group with a prefix. For example, you can deny all JVM metrics with `MeterFilter.denyNameStartsWith("jvm")`
- `deny(Predicate)` - Deny any meter matching the predicate.
- `maximumAllowableMetrics(int)` - Denies any meter after the registry has reached a certain number of meters.
- `maximumAllowableTags(String meterNamePrefix, String tagKey, int maximumTagValues, MeterFilter onMaxReached)` - Places an upper bound on the number of tags produced by matching series.

**Whitelisting** only a certain group of metrics is a particularly common case for monitoring systems that are *expensive*. This can be achieved with the static:

- `denyUnless(Predicate)` - Deny all meters that *don’t* match the predicate.

#### 6.1.2. Chaining deny/accept meters

Meter filters are applied in the order in which they are configured on the registry, so it is possible to stack deny/accept filters to achieve more complex rules:

```java
registry.config()
    .meterFilter(MeterFilter.acceptNameStartsWith("http"))
    .meterFilter(MeterFilter.deny()); (1)
```

This achieves another form of whitelisting by stacking two filters together. Only "http" metrics will exist in this registry.

### 6.2. Transforming metrics

A transform filter looks like this:

```java
new MeterFilter() {
    @Override
    public Meter.Id map(Meter.Id id) {
       if(id.getName().startsWith("test")) {
          return id.withName("extra." + id.getName()).withTag("extra.tag", "value");
       }
       return id;
    }
}
```

This filter adds a name prefix and an additional tag conditionally to meters starting with the name "test".

`MeterFilter` provides convenience builders for many common transformation cases:

- `commonTags(Iterable)` - Add a set of tags to all metrics. Adding common tags for app name, host, region, etc is a highly recommended practice.
- `ignoreTags(String…)` - Drop matching tag keys from every meter. This is particularly useful when a tag provably becomes too high cardinality and starts stressing your monitoring system or costing too much, but you can’t change all the instrumentation points quickly.
- `replaceTagValues(String tagKey, Function replacement, String… exceptions)` - Replace tag values according to the provided mapping for all matching tag keys. This can be used to reduce the total cardinality of a tag by mapping some portion of tag values to something else.
- `renameTag(String meterNamePrefix, String fromTagKey, String toTagKey)` - Rename a tag key for every metric beginning with a given prefix.

### 6.3. Configuring distribution statistics

`Timer` and `DistributionSummary` contain a set of optional distribution statistics in addition to the basics of count, total, and max that can be configured through filters. These distribution statistics include pre-computed percentiles, SLAs, and histograms.

```java
new MeterFilter() {
    @Override
    public DistributionStatisticConfig configure(Meter.Id id, DistributionStatisticConfig config) {
        if (id.getName().startsWith(prefix)) {
            return DistributionStatisticConfig.builder()
                    .publishPercentiles(0.9, 0.95)
                    .build()
                    .merge(config);
        }
        return config;
    }
};
```

Generally, you should create a new `DistributionStatisticConfig` with just the pieces you wish to configure and then `merge` it with the input configuration. This allows you to drop-down on registry provided defaults for distribution statistics and to chain multiple filters together, each which configures some part of the distribution statistics (e.g. maybe you want a 100ms SLA for all http requests but only percentile histograms on a few critical endpoints).

`MeterFilter` provides convenience builders for:

- `maxExpected(Duration/long)` - Governs the upper bound of percentile histogram buckets shipped from a timer or summary.
- `minExpected(Duration/long)` - Governs the lower bound of percentile histogram buckets shipped from a timer or summary.

Spring Boot offers property-based filters for configuring SLAs, percentiles, and percentile histograms by name prefix.

## 7. Rate aggregation

Micrometer is aware of whether a particular monitoring system expects rate aggregation to happen client-side before metrics are published or ad-hoc as part of the query on the server-side. It accumulates metrics according to which style the monitoring system expects.

Not all measurements are reported or best viewed as a rate. For example, gauge values and long task timer active tasks are not rates.

### 7.1. Server-side

Monitoring systems that perform server-side rate math expect absolute values to be reported at each publishing interval. For example, the absolute count of all increments to a counter since the beginning of the application is sent on each publishing interval.

Suppose we have a slightly-positively biased random walk that chooses to increment a counter once every 10 milliseconds. If we view the raw counter value in a system like Prometheus, we see a step-wise monotonically increasing function (the width of the step is the interval at which Prometheus is polling or scraping for data).

![绝对计数器值” class =“ img-fluid](http://micrometer.io/2e17a23d9468da4e32bac1d368a18b2e.png)

Representing a counter without rate aggregation over some time window is rarely useful, as the representation is a function of both the rapidity with which the counter is incremented and the longevity of the service. In our example above, the counter drops back to zero on service restart. The rate-aggregated graph would return back to a value around 55 as soon as the new instance (say on a production deployment) was in service.

![Rate-aggregated counter" class="img-fluid](http://micrometer.io/f12ca6e6f2bed6fa80c9fe9d2af1a9a5.png)

If you have achieved zero-downtime deployments (e.g. through red-black deployments), you should be able to comfortably set *minimum* alert thresholds on the rate-aggregated graph without service restarts causing dips in the counter value.

| IMPORTANT | For most production purposes whether it be alerting, automated canary analysis, etc. base your automation off of rate-aggregated data. |
| --------- | ------------------------------------------------------------ |
|           |                                                              |

### 7.2. Client-side

Another class of monitoring system either:

1. Expects rate-aggregated data. Given the key insight that for most production purposes, we should be basing our decisions off of rates rather than absolute values, such systems benefit from having to do less math to satisfy queries.
2. Has relatively little or no math operations that would allow us to rate-aggregate data through our queries. For these systems, publishing pre-aggregated data is the only way to build meaningful representations.

Micrometer efficiently maintains rate data by means of a step value that accumulates data for the current publishing interval. When the step value is polled (when publishing for example), if the step value detects that the current interval has elapsed, it moves current data to "previous" state. This previous state is what is reported until the next time current data overwrites it. Below is an illustration of the interaction of current and previous state along with polling:

![Behavior of a step value" class="img-fluid](http://micrometer.io/d81a710ce37b5a4443ed2e4f0146143c.png)

The value returned by the poll function is always a *rate per second \* interval*. If the step value illustrated above represents the values of a counter, we could say that the counter saw "0.3 increments per second" in the first interval, which is reportable to the backend at any time during the second interval.

Micrometer timers track at least a count and total time as separate measurements. Suppose we configure publishing at 10 second intervals and we saw 20 requests that each took 100ms. Then for the first interval:

1. `count` = 10 seconds * (20 requests / 10 seconds) = 20 requests
2. `totalTime` = 10 seconds * (20 * 100 ms / 10 seconds) = 2 seconds

The `count` statistic is meaningful standing alone — it is a measure of *throughput*. `totalTime` represents the total latency of all requests in the interval. Additionally:

`totalTime / count` = 2 seconds / 20 requests = 0.1 seconds / request = 100 ms / request

This is a useful measure of *average latency*. When the same idea is applied to the `totalAmount` and `count` emanating from distribution summaries, the measure is called a *distribution average*. Average latency is just the distribution average for a distribution summary measured in time (a timer). Some monitoring systems like Atlas provide facilities for computing the distribution average from these statistics, and Micrometer will ship `totalTime` and `count` as separate statistics. Others like Datadog don’t have this kind of operation built-in, and Micrometer will calculate the distribution average client-side and ship that.

Shipping the rate for the publishing interval is sufficient to reason about the rate over any time window greater than or equal to the publishing interval. In our example, if a service continues to receive 20 requests each taking 100ms for every 10 second interval in a given minute, then we could say:

1. Micrometer reported "20 requests" for `count` on every 10 second interval. The monitoring system simply sums these six 10 second intervals and arrives at the conclusion that there are 120 requests / minute. Note that it is the monitoring system doing this summation, not Micrometer.
2. Micrometer reported "2 seconds" of `totalTime` on every 10 second interval. The monitoring system can sum all total time statistics over the minute to yield "12 seconds" of total time in the minute interval. Then, the average latency is just as we expect: 12 seconds / 120 requests = 100 ms / request.

## 8. Counters

Counters report a single metric, a count. The `Counter` interface allows you to increment by a fixed amount, which must be positive.

| TIP  | Never count something you can time with a `Timer` or summarize with a `DistributionSummary`! Both `Timer` and `DistributionSummary` always publish a count of events in addition to other measurements. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

When building graphs and alerts off of counters, generally you should be most interested in measuring the rate at which some event is occurring over a given time interval. Consider a simple queue. Counters could be used to measure things like the rate at which items are being inserted and removed.

It’s tempting at first to conceive of visualizing absolute counts rather than a rate, but the absolute count is usually both a function of the rapidity with which something is used **and** the longevity of the application instance under instrumentation. Building dashboards and alerts of the rate of a counter per some interval of time disregards the longevity of the app, letting you see aberrant behavior long after the app has started.

| NOTE | Be sure to read through the timer section before jumping into using counters, as timers record a count of timed events as part of the suite of metrics that go into timing. For those pieces of code you intend to time, you do NOT need to add a counter separately. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

The following code simulates a real counter whose rate exhibits some perturbation over a short time window.

```java
Normal rand = ...; // a random generator

MeterRegistry registry = ...
Counter counter = registry.counter("counter"); (1)

Flux.interval(Duration.ofMillis(10))
        .doOnEach(d -> {
            if (rand.nextDouble() + 0.1 > 0) { (2)
                counter.increment(); (3)
            }
        })
        .blockLast();
```

1. Most counters can be created off of the registry itself with a name and, optionally, a set of tags.
2. A slightly positively-biased random walk.
3. This is how you interact with a counter. You could also call `counter.increment(n)` to increment by more than 1 in a single operation.

There is also a fluent builder for counters on the `Counter` interface itself, providing access to less frequently used options like base units and description. You can register the counter as the last step of its construction by calling `register`.

```java
Counter counter = Counter
    .builder("counter")
    .baseUnit("beans") // optional
    .description("a description of what this counter does") // optional
    .tags("region", "test") // optional
    .register(registry);
```

### 8.1. Function-tracking counters

Micrometer also provides a more infrequently used counter pattern that tracks a monotonically increasing function (a function that stays the same or increases over time, but never decreases). Some monitoring systems, like Prometheus, push cumulative values for counters to the backend, but others publish the rate at which a counter is incrementing over the push interval. By employing this pattern, you allow the Micrometer implementation for your monitoring system to choose whether to rate normalize the counter or not and your counter remains portable across different types of monitoring systems.

```java
Cache cache = ...; // suppose we have a Guava cache with stats recording on
registry.more().counter("evictions", tags, cache, c -> c.stats().evictionCount()); (1)
```

1. `evictionCount()` is a monotonically increasing function incrementing with every cache eviction from the beginning of its life.

The function-tracking counter, in concert with the monitoring system’s rate normalizing functionality (whether this is an artifact of the query language or the way data is pushed to the system), adds a layer of richness on top of the cumulative value of the function itself. You can reason about the *rate* at which the value is increasing, whether that rate is within an acceptable bound, is increasing or decreasing over time, etc.

| WARNING | Micrometer cannot guarantee the monotonicity of the function for you. By using this signature, you are asserting its monotonicity based on what you know about its definition. |
| ------- | ------------------------------------------------------------ |
|         |                                                              |

There is also a fluent builder for function counters on the `FunctionCounter` interface itself, providing access to less frequently used options like base units and description. You can register the counter as the last step of its construction by calling `register(MeterRegistry)`.

```java
MyCounterState state = ...;

FunctionCounter counter = FunctionCounter
    .builder("counter", state, state -> state.count())
    .baseUnit("beans") // optional
    .description("a description of what this counter does") // optional
    .tags("region", "test") // optional
    .register(registry);
```

## 9. Gauges

A gauge is a handle to get the current value. Typical examples for gauges would be the size of a collection or map or number of threads in a running state.

| TIP  | Gauges are useful for monitoring things with natural upper bounds. We don’t recommend using a gauge to monitor things like request count, as they can grow without bound for the duration of an application instance’s life. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| TIP  | Never gauge something you can count with a `Counter`! |
| ---- | ----------------------------------------------------- |
|      |                                                       |

Micrometer takes the stance that gauges should be sampled and not be set, so there is no information about what might have occurred between samples. After all, any intermediate values set on a gauge are lost by the time the gauge value is reported to a metrics backend anyway, so there seems to be little value in setting those intermediate values in the first place.

If it helps, think of a `Gauge` as a "heisen-gauge" - a meter that only changes when it is observed. Every other meter type provided out-of-the-box accumulates intermediate counts toward the point where the data is sent to the metrics backend.

The `MeterRegistry` interface contains methods for building gauges to observe numeric values, functions, collections, and maps.

```java
List<String> list = registry.gauge("listGauge", Collections.emptyList(), new ArrayList<>(), List::size); (1)
List<String> list2 = registry.gaugeCollectionSize("listSize2", Tags.empty(), new ArrayList<>()); (2)
Map<String, Integer> map = registry.gaugeMapSize("mapGauge", Tags.empty(), new HashMap<>());
```

1. A slightly more common form of gauge is one that monitors some non-numeric object. The last argument establishes the function that is used to determine the value of the gauge when the gauge is observed.
2. A more convenient form of (1) for when you simply want to monitor collection size.

All of the different forms of creating a gauge maintain only a *weak reference* to the object being observed, so as not to prevent garbage collection of the object.

### 9.1. Manually incrementing/decrementing a Gauge

A gauge can be made to track any `java.lang.Number` subtype that is settable, such as `AtomicInteger` and `AtomicLong` found in `java.util.concurrent.atomic` and similar types like Guava’s `AtomicDouble`.

```java
// maintain a reference to myGauge
AtomicInteger myGauge = registry.gauge("numberGauge", new AtomicInteger(0));

// ... elsewhere you can update the value it holds using the object reference
myGauge.set(27);
myGauge.set(11);
```

Note that in this form unlike other meter types you don’t get a reference to the `Gauge` when creating one, but rather the thing being observed. This is because of the heisen-gauge principal; the gauge is self sufficient once created, so you should never need to interact with it. This allows us to give you back only the instrumented object, which allows for quick one liners that both create the object to be observed and set up metrics around it.

This pattern should be less common than the `DoubleFunction` form. Remember that frequent setting of the observed `Number` results in a lot of intermediate values that never get published. Only the *instantaneous value* of the gauge at publish time is ever sent to the monitoring system.

| WARNING | Attempting to construct a gauge with a primitive number or one of its `java.lang` object forms is always incorrect. These numbers are immutable, and thus the gauge cannot ever be changed. Attempting to "re-register" the gauge with a different number won’t work, as the registry only maintains one meter for each unique combination of name and tags. |
| ------- | ------------------------------------------------------------ |
|         |                                                              |

### 9.2. Gauge fluent builder

The interface contains a fluent builder for gauges:

```java
Gauge gauge = Gauge
    .builder("gauge", myObj, myObj::gaugeValue)
    .description("a description of what this gauge does") // optional
    .tags("region", "test") // optional
    .register(registry);
```

Generally the returned `Gauge` instance is not useful except in testing, as the gauge is already set up to track a value automatically upon registration.

### 9.3. Why is my Gauge reporting NaN or disappearing?

It is your responsibility to hold a strong reference to the state object that you are measuring with a `Gauge`. Micrometer is careful to not create strong references to objects that would otherwise be garbage collected. Once the object being gauged is de-referenced and is garbage collected, Micrometer will start reporting a NaN or nothing for a gauge, depending on the registry implementation.

If you see your gauge reporting for a few minutes and then disappearing or reporting NaN, it almost certainly suggests that the underlying object being gauged has been garbage collected.

## 10. Timers

Timers are intended for measuring short-duration latencies, and the frequency of such events. All implementations of `Timer` report at least the total time and count of events as separate time series. While Timers can be used for other use cases, note negative values are not supported, and recording many longer durations could cause overflow of the total time at `Long.MAX_VALUE` nanoseconds (292.3 years).

As an example, consider a graph showing request latency to a typical web server. The server can be expected to respond to many requests quickly, so the timer will be getting updated many times per second.

The appropriate base unit for timers varies by metrics backend for good reason. Micrometer is decidedly un-opinionated about this, but because of the potential for confusion, requires a `TimeUnit` when interacting with `Timer`s. Micrometer is aware of the preferences of each implementation and publishes your timing in the appropriate base unit based on the implementation.

```java
public interface Timer extends Meter {
    ...
    void record(long amount, TimeUnit unit);
    void record(Duration duration);
    double totalTime(TimeUnit unit);
}
```

The interface contains a fluent builder for timers:

```java
Timer timer = Timer
    .builder("my.timer")
    .description("a description of what this timer does") // optional
    .tags("region", "test") // optional
    .register(registry);
```

| NOTE | Max for basic `Timer` implementations such as `CumulativeTimer`, `StepTimer` is a time window max (`TimeWindowMax`). It means that its value is the maximum value during a time window. If no new values are recorded for the time window length, the max will be reset to 0 as a new time window starts. Time window size will be the step size of the meter registry unless expiry in `DistributionStatisticConfig` is set to other value explicitly. The reason why a time window max is used is to capture max latency in a subsequent interval after heavy resource pressure triggers the latency and prevents metrics from being published. Percentiles are also a time window percentiles (`TimeWindowPercentileHistogram`). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 10.1. Recording blocks of code

The `Timer` interface exposes several convenience overloads for recording timings inline, e.g.:

```java
timer.record(() -> dontCareAboutReturnValue());
timer.recordCallable(() -> returnValue());

Runnable r = timer.wrap(() -> dontCareAboutReturnValue()); (1)
Callable c = timer.wrap(() -> returnValue());
```

1. Wrap `Runnable` or `Callable` and return the instrumented version of it for use later.

| NOTE | A `Timer` is really just a specialized distribution summary that is aware of how to scale durations to the base unit of time of each monitoring system and has an automatically determined base unit. In every case where you want to measure time, you should use a `Timer` rather than a `DistributionSummary`. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 10.2. Storing start state in `Timer.Sample`

You may also store start state in a sample instance that can be stopped later. The sample records a start time based on the registry’s clock. After starting a sample, execute the code to be timed and finish the operation by calling `stop(Timer)` on the sample.

```java
Timer.Sample sample = Timer.start(registry);

// do stuff
Response response = ...

sample.stop(registry.timer("my.timer", "response", response.status()));
```

Note how we don’t decide what timer we are accumulating the sample to until it is time to stop the sample. This allows us to dynamically determine certain tags from the end state of the operation we are timing.

### 10.3. The `@Timed` annotation

The `micrometer-core` modules contains a `@Timed` annotation that can be used by frameworks to add timing support to either specific types of methods such as those serving web request endpoints or generally to all methods.

| WARNING | Micrometer’s Spring Boot configuration does *not* recognize `@Timed` on arbitrary methods. |
| ------- | ------------------------------------------------------------ |
|         |                                                              |

An incubating AspectJ aspect is included in `micrometer-core` as well that you can use in your application either through compile/load time AspectJ weaving or through framework facilities that interpret AspectJ aspects and proxy targeted methods in some other way, such as Spring AOP. Here is a sample Spring AOP configuration:

```java
@Configuration
public class TimedConfiguration {
   @Bean
   public TimedAspect timedAspect(MeterRegistry registry) {
      return new TimedAspect(registry);
   }
}
```

Applying `TimedAspect` makes `@Timed` usable on any arbitrary method in an AspectJ proxied instance, e.g.:

```java
@Service
public class ExampleService {

  @Timed
  public void sync() {
    // @Timed will record the execution time of this method,
    // from the start and until it exists normally or exceptionally.
    ...
  }

  @Async
  @Timed
  public CompletableFuture<?> async() {
    // @Timed will record the execution time of this method,
    // from the start and until the returned CompletableFuture
    // completes normally or exceptionally.
    return CompletableFuture.supplyAsync(...);
  }

}
```

### 10.4. Function-tracking timers

Micrometer also provides a more infrequently used timer pattern that tracks two monotonically increasing functions (a function that stays the same or increases over time, but never decreases): a count function and a total time function. Some monitoring systems, like Prometheus, push cumulative values for counters (which apply to both the count and total time functions in this case) to the backend, but others publish the rate at which a counter is incrementing over the push interval. By employing this pattern, you allow the Micrometer implementation for your monitoring system to choose whether to rate normalize the timer or not and your timer remains portable across different types of monitoring systems.

```java
IMap<?, ?> cache = ...; // suppose we have a Hazelcast cache
registry.more().timer("cache.gets.latency", Tags.of("name", cache.getName()), cache,
    c -> c.getLocalMapStats().getGetOperationCount(), (1)
    c -> c.getLocalMapStats().getTotalGetLatency(),
    TimeUnit.NANOSECONDS (2)
);
```

1. `getGetOperationCount()` is a monotonically increasing function incrementing with every cache get from the beginning of its life.
2. This represents the unit of time represented by `getTotalGetLatency()`. Each registry implementation specifies what its expected base unit of time is, and the total time reported will be scaled to this value.

The function-tracking timer, in concert with the monitoring system’s rate normalizing functionality (whether this is an artifact of the query language or the way data is pushed to the system), adds a layer of richness on top of the cumulative value of the functions themselves. You can reason about the *rate* of throughput and latency, whether that rate is within an acceptable bound, is increasing or decreasing over time, etc.

| WARNING | Micrometer cannot guarantee the monotonicity of the count and total time functions for you. By using this signature, you are asserting their monotonicity based on what you know about their definitions. |
| ------- | ------------------------------------------------------------ |
|         |                                                              |

There is also a fluent builder for function timers on the `FunctionTimer` interface itself, providing access to less frequently used options like base units and description. You can register the timer as the last step of its construction by calling `register(MeterRegistry)`.

```java
IMap<?, ?> cache = ...

FunctionTimer.builder("cache.gets.latency", cache,
        c -> c.getLocalMapStats().getGetOperationCount(),
        c -> c.getLocalMapStats().getTotalGetLatency(),
        TimeUnit.NANOSECONDS)
    .tags("name", cache.getName())
    .description("Cache gets")
    .register(registry);
```

### 10.5. Pause detection

Micrometer uses the LatencyUtils package to compensate for [coordinated omission](https://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html) — extra latency arising from system and VM pauses that skew your latency statistics downward. Distribution statistics like percentiles and SLA counts are influenced by a pause detector implementation that adds additional latency here and there to compensate for pauses.

Micrometer supports two pause detector implementations: a clock-drift based detector and a no-op detector. Before Micrometer 1.0.10/1.1.4/1.2.0, a clock-drift detector was configured by default to report as-accurate-as-possible metrics without further configuration. Since 1.0.10/1.1.4/1.2.0, the no-op detector is configured by default, but the clock-drift detector can be configured as shown below.

The clock-drift based detector has a configurable sleep interval and pause threshold. CPU consumption is inversely proportional to `sleepInterval`, as is pause detection accuracy. 100ms for both values is a reasonable default to offer decent detection of long pause events while consuming a negligible amount of CPU time.

You can customize the pause detector using:

```java
registry.config().pauseDetector(new ClockDriftPauseDetector(sleepInterval, pauseThreshold));
registry.config().pauseDetector(new NoPauseDetector());
```

In the future, we may provide further detector implementations. Some pauses may be able to be inferred from GC logging in some circumstances, for example, without requiring a constant CPU load however minimal. It’s also possible that a future JDK will provide direct access to pause events.

### 10.6. Memory footprint estimation

Timers are the most memory-consuming meter, and their total footprint can vary dramatically depending on which options you choose. Below is a table of memory consumption based on the use of various features. These figures assume no tags and a ring buffer length of 3. Adding tags of course adds somewhat to the total, as does increasing the buffer length. Total storage can also vary somewhat depending on the registry implementation.

- R = Ring buffer length. We assume the default of 3 in all examples. R is set with `Timer.Builder#distributionStatisticBufferLength`.
- B = Total histogram buckets. Can be SLA boundaries or percentile histogram buckets. By default, timers are clamped to a minimum expected value of 1ms and a maximum expected value of 30 seconds, yielding 66 buckets for percentile histograms, when applicable.
- I = Interval estimator for pause compensation. 1.7 kb
- M = Time-decaying max. 104 bytes
- Fb = Fixed boundary histogram. 30b * B * R
- Pp = Percentile precision. By default is 1. Generally in the range [0, 3]. Pp is set with `Timer.Builder#percentilePrecision`.
- Hdr(Pp) = High dynamic range histogram.
  - When Pp = 0: 1.9kb * R + 0.8kb
  - When Pp = 1: 3.8kb * R + 1.1kb
  - When Pp = 2: 18.2kb * R + 4.7kb
  - When Pp = 3: 66kb * R + 33kb

| Pause detection | Client-side percentiles | Histogram and/or SLAs | Formula         | Example                                                      |
| :-------------- | :---------------------- | :-------------------- | :-------------- | :----------------------------------------------------------- |
| Yes             | No                      | No                    | I + M           | ~1.8kb                                                       |
| Yes             | No                      | Yes                   | I + M + Fb      | For default percentile histogram, ~7.7kb                     |
| Yes             | Yes                     | Yes                   | I + M + Hdr(Pp) | For the addition of a 0.95 percentile with defaults otherwise, ~14.3kb |
| No              | No                      | No                    | M               | ~0.1kb                                                       |
| No              | No                      | Yes                   | M + Fb          | For default percentile histogram, ~6kb                       |
| No              | Yes                     | Yes                   | M + Hdr(Pp)     | For the addition of a 0.95 percentile with defaults otherwise, ~12.6kb |

| NOTE | For Prometheus specifically, R is *always* equal to 1, regardless of how you attempt to configure it through `Timer.Builder`. This is special-cased for Prometheus because it expects cumulative histogram data that never rolls over. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 11. Distribution summaries

A distribution summary is used to track the distribution of events. It is similar to a timer structurally, but records values that do not represent a unit of time. For example, a distribution summary could be used to measure the payload sizes of requests hitting a server.

To create a distribution summary:

```java
DistributionSummary summary = registry.summary("response.size");
```

The interface contains a fluent builder for distribution summaries:

```java
DistributionSummary summary = DistributionSummary
    .builder("response.size")
    .description("a description of what this summary does") // optional
    .baseUnit("bytes") // optional (1)
    .tags("region", "test") // optional
    .scale(100) // optional (2)
    .register(registry);
```

1. Add base units for maximum portability — base units are part of the naming convention for some monitoring systems. Leaving it off and violating the naming convention will have no adverse effect if you forget.
2. Optionally, you may provide a scaling factor that each recorded sample will be multiplied by as it is recorded.

| NOTE | Max for basic `DistributionSummary` implementations such as `CumulativeDistributionSummary`, `StepDistributionSummary` is a time window max (`TimeWindowMax`). It means that its value is the maximum value during a time window. If no new values are recorded for the time window length, the max will be reset to 0 as a new time window starts. Time window size will be the step size of the meter registry unless expiry in `DistributionStatisticConfig` is set to other value explicitly. The reason why a time window max is used is to capture max latency in a subsequent interval after heavy resource pressure triggers the latency and prevents metrics from being published. Percentiles are also a time window percentiles (`TimeWindowPercentileHistogram`). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 11.1. Scaling and histograms

Micrometer’s preselected percentile histogram buckets are all integers from 1 to maximum long. Currently `minimumExpectedValue` and `maximumExpectedValue` serve to control the cardinality of the bucket set. If we attempt to detect that your min/max yields a small range and scale the preselected bucket domain to your summary’s range, then we don’t have another lever to control bucket cardinality.

Instead, if your summary’s domain is more constrained, scale your summary’s range by a fixed factor. The use case we’ve heard so far is for summaries of ratios whose domain is [0,1]. Then:

```java
DistributionSummary.builder("my.ratio").scale(100).register(registry)
```

This way, the ratio winds up in the range [0,100] and we can set `maximumExpectedValue` to 100. Pair this with custom SLA boundaries if you care about particular ratios:

```java
DistributionSummary.builder("my.ratio")
   .scale(100)
   .sla(70, 80, 90)
   .register(registry)
```

### 11.2. Memory footprint estimation

The total memory footprint of a distribution summary can vary dramatically depending on which options you choose. Below is a table of memory consumption based on the use of various features. These figures assume no tags and a ring buffer length of 3. Adding tags of course adds somewhat to the total, as does increasing the buffer length. Total storage can also vary somewhat depending on the registry implementation.

- R = Ring buffer length. We assume the default of 3 in all examples. R is set with `DistributionSummary.Builder#distributionStatisticBufferLength`.
- B = Total histogram buckets. Can be SLA boundaries or percentile histogram buckets. By default, summaries have NO minimum and maximum expected value, so ship all 276 predetermined histogram buckets. You should always clamp distribution summaries with a `minimumExpectedValue` and `maximumExpectedValue` when you intend to ship percentile histograms.
- M = Time-decaying max. 104 bytes
- Fb = Fixed boundary histogram. 30b * B * R
- Pp = Percentile precision. By default is 1. Generally in the range [0, 3]. Pp is set with `DistributionSummary.Builder#percentilePrecision`.
- Hdr(Pp) = High dynamic range histogram.
  - When Pp = 0: 1.9kb * R + 0.8kb
  - When Pp = 1: 3.8kb * R + 1.1kb
  - When Pp = 2: 18.2kb * R + 4.7kb
  - When Pp = 3: 66kb * R + 33kb

| Client-side percentiles | Histogram and/or SLAs | Formula     | Example                                                      |
| :---------------------- | :-------------------- | :---------- | :----------------------------------------------------------- |
| No                      | No                    | M           | ~0.1kb                                                       |
| No                      | Yes                   | M + Fb      | For percentile histogram clamped to 66 buckets, ~6kb         |
| Yes                     | Yes                   | M + Hdr(Pp) | For the addition of a 0.95 percentile with defaults otherwise, ~12.6kb |

| NOTE | For Prometheus specifically, R is *always* equal to 1, regardless of how you attempt to configure it through `DistributionSummary.Builder`. This is special-cased for Prometheus because it expects cumulative histogram data that never rolls over. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 12. Long task timers

The long task timer is a special type of timer that lets you measure time while an event being measured is **still running**. A timer does not record the duration until the task is complete.

Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups, and others . Normally all data can be refreshed in a few minutes. If the AWS services have problems, it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata.

For example, in a Spring application, it is common for such long running processes to be implemented with `@Scheduled`. Micrometer provides a special `@Timed` annotation for instrumenting these processes with a long task timer.

```java
@Timed(value = "aws.scrape", longTask = true)
@Scheduled(fixedDelay = 360000)
void scrapeResources() {
    // find instances, volumes, auto-scaling groups, etc...
}
```

It is up to the application framework to make something happen with `@Timed`. If your framework of choice does not support it, you can still use the long task timer:

```java
LongTaskTimer scrapeTimer = registry.more().longTaskTimer("scrape");
void scrapeResources() {
    scrapeTimer.record(() => {
        // find instances, volumes, auto-scaling groups, etc...
    });
}
```

If we wanted to alert when this process exceeds threshold, with a long task timer we will receive that alert at the first reporting interval after we have exceeded the threshold. With a regular timer, we wouldn’t receive the alert until the first reporting interval after the process completed, over an hour later!

The interface contains a fluent builder for long task timers:

```java
LongTaskTimer longTaskTimer = LongTaskTimer
    .builder("long.task.timer")
    .description("a description of what this timer does") // optional
    .tags("region", "test") // optional
    .register(registry);
```

## 13. Histograms and percentiles

Timers and distribution summaries support collecting data to observe their percentile distributions. There are two main approaches to viewing percentiles:

1. **Percentile histograms** - Micrometer accumulates values to an underlying histogram and ships a predetermined set of buckets to the monitoring system. The monitoring system’s query language is responsible for calculating percentiles off of this histogram. Currently, only Prometheus, Atlas, and Wavefront support histogram-based percentile approximations, via `histogram_quantile`, `:percentile`, and `hs()` respectively. If targeting Prometheus, Atlas, or Wavefront, prefer this approach, since you can aggregate the histograms across dimensions (by simply summing the values of the buckets across a set of dimensions) and derive an aggregable percentile from the histogram.
2. **Client-side percentiles** - Micrometer computes a percentile approximation for each meter ID (set of name and tags) and ships the percentile value to the monitoring system. This is not as flexible as a percentile histogram because it is not possible to aggregate percentile approximations across tags. Nevertheless, it provides some level of insight into percentile distributions for monitoring systems that don’t support server-side percentile calculation based on a histogram.

Here is an example of building a timer with a histogram:

```java
Timer.builder("my.timer")
   .publishPercentiles(0.5, 0.95) // median and 95th percentile
   .publishPercentileHistogram()
   .sla(Duration.ofMillis(100))
   .minimumExpectedValue(Duration.ofMillis(1))
   .maximumExpectedValue(Duration.ofSeconds(10))
```

1. `publishPercentiles` - This is used to publish percentile values computed in your app. These values are non-aggregable across dimensions.
2. `publishPercentileHistogram` - This is used to publish a histogram suitable for computing aggregable (across dimensions) percentile approximations in Prometheus using `histogram_quantile`, Atlas using `:percentile`, and Wavefront using `hs()`. For Prometheus and Atlas, the buckets in the resulting histogram are preset by Micrometer based on a generator that has been determined empirically by Netflix to yield a reasonable error bound on most real world timers and distribution summaries. The generator yields 276 buckets by default, but Micrometer only ships those that are within the range set by `minimumExpectedValue` and `maximumExpectedValue`, inclusive. Micrometer clamps timers by default to a range of 1 millisecond to 1 minute, yielding 73 histogram buckets per timer dimension. `publishPercentileHistogram` has no effect on systems that do not support aggregable percentile approximations — no histogram is shipped for these systems.
3. `sla` - Publish a cumulative histogram with buckets defined by your SLAs. Used in concert with `publishPercentileHistogram` on a monitoring system that supports aggregable percentiles, this setting adds additional buckets to the published histogram. Used on a system that does not support aggregable percentiles, this setting causes a histogram to be published with only these buckets.
4. `minimumExpectedValue`/`maximumExpectedValue` - Controls the number of buckets shipped by `publishPercentileHistogram` as well as controlling the accuracy and memory footprint of the underlying HdrHistogram structure.

由于将百分位数运送到监视系统会生成其他时间序列，因此通常最好**不要**在作为应用程序依赖项包含的核心库中对其进行配置。取而代之的是，应用程序可以通过仪表过滤器为某些计时器/分配摘要集启用此行为。

例如，假设我们在一个公共库中有几个计时器。我们为这些计时器名称添加了前缀`myservice`：

```java
registry.timer("myservice.http.requests").record(..);
registry.timer("myservice.db.requests").record(..);
```

我们可以通过仪表过滤器为两个计时器打开客户端百分位数：

```java
registry.config().meterFilter(
    new MeterFilter() {
        @Override
        public DistributionStatisticConfig configure(Meter.Id id, DistributionStatisticConfig config) {
            if(id.getName().startsWith("myservice")) {
                return DistributionStatisticConfig.builder()
                    .percentiles(0.95)
                    .build()
                    .merge(config);
            }
            return config;
        }
    });
```

------

[1](http://micrometer.io/docs/concepts#_footnoteref_1)。从1.2.0开始，Micrometer将累积值发送到Wavefront。

©2017-2020 Pivotal Software，Inc.保留所有权利。请参阅[使用条款](https://www.pivotal.io/terms-of-use)和[隐私政策](https://www.pivotal.io/privacy-policy)。